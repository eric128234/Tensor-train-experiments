{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "787f0689-9fae-40a9-93c6-707d63c93edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F(0,1,2,0) = -1.0102064154018084\n",
      "Full tensor shape: (3, 3, 3, 3)\n",
      "Check same entry: -1.0102064154018084\n",
      "Max abs diff (20 samples): 2.7755575615628914e-16\n"
     ]
    }
   ],
   "source": [
    "#check the efficiency on different tensors cigkl stiffness tensor, sometimes the order of the index affect the efficiency of the compressness (didnt do prrLU on it) maybe due to some other TCI method choice?\n",
    "prrlu_complete_pivot_LDU = prrlu_complete_pivot_LDU_direct\n",
    "import itertools\n",
    "import random\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, List, Sequence, Tuple, Optional\n",
    "\n",
    "Array = np.ndarray\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# TT core utilities\n",
    "# ----------------------------\n",
    "\n",
    "def tt_random_cores(dims: Sequence[int], ranks: Sequence[int], seed: int = 0) -> List[Array]:\n",
    "    \"\"\"\n",
    "    Create random TT cores with shapes (r_{k-1}, n_k, r_k).\n",
    "    ranks must have length= number of legs+1 with ranks[0]=ranks[-1]=1.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    N = len(dims)\n",
    "    assert len(ranks) == N + 1 #Number of ranks (since the first core need 0th rank as well)\n",
    "    assert ranks[0] == 1 and ranks[-1] == 1\n",
    "    cores = []\n",
    "    for k, n in enumerate(dims): #enumerate loop through each leg automatically and create N cores\n",
    "        rL, rR = ranks[k], ranks[k+1]\n",
    "        cores.append(rng.standard_normal((rL, n, rR))) #the 3D array core\n",
    "    return cores\n",
    "\n",
    "\n",
    "def tt_eval_entry(cores: List[Array], index: Sequence[int]) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate a single tensor entry given TT cores at a multi-index.\n",
    "    index length must equal number of cores.\n",
    "    \"\"\"\n",
    "    N = len(cores)\n",
    "    assert len(index) == N\n",
    "    v = cores[0][0, index[0], :]          # (r1,) v is the current dimension of the matrix slice multiplication, equal to the rank of current SVD matrix\n",
    "    for k in range(1, N-1):\n",
    "        v = v @ cores[k][:, index[k], :]  # from kth core we fix index of the input sigma_k and get 2D slice (r_{k+1},)\n",
    "    v = v @ cores[-1][:, index[-1], 0]    # scalar\n",
    "    return float(v)\n",
    "\n",
    "\n",
    "def tt_contract_all(cores: List[Array]) -> Array:\n",
    "    \"\"\"\n",
    "    Materialize full tensor from TT cores (only for small dims).\n",
    "    Returns ndarray with shape dims.\n",
    "    \"\"\"\n",
    "    N = len(cores)\n",
    "    dims = [G.shape[1] for G in cores]\n",
    "    # Start with first core: shape (n1, r1)\n",
    "    X = cores[0][0, :, :]  # (n1, r1)\n",
    "    for k in range(1, N):\n",
    "        G = cores[k]  # (rL, nk, rR)\n",
    "        # Contract X (..., rL) with G (rL, nk, rR) -> (..., nk, rR)\n",
    "        X = np.tensordot(X, G, axes=([-1], [0]))\n",
    "        # Now X shape: (*prev_dims, nk, rR)\n",
    "    # Final rank should be 1\n",
    "    assert X.shape[-1] == 1\n",
    "    return np.squeeze(X, axis=-1)  # shape dims\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Tensor Oracle interface\n",
    "# ----------------------------\n",
    "\n",
    "@dataclass\n",
    "@dataclass\n",
    "class TensorOracle:\n",
    "    dims: Tuple[int, ...] #store dim as tuple of integers\n",
    "    entry: Callable[[Tuple[int, ...]], float]\n",
    "    n_calls: int = 0\n",
    "\n",
    "    def __call__(self, idx: Sequence[int]) -> float:\n",
    "        self.n_calls += 1\n",
    "        idx = tuple(int(i) for i in idx) # convert the input indices for each leg to tuple for oracle to evaluate\n",
    "        return self.entry(idx)\n",
    "\n",
    "    def reset_counter(self):\n",
    "        self.n_calls = 0\n",
    "\n",
    "\n",
    "    def block(self,\n",
    "              I_left: Sequence[Tuple[int, ...]],\n",
    "              S_mid: Sequence[Tuple[int, ...]],\n",
    "              J_right: Sequence[Tuple[int, ...]]) -> Array:\n",
    "        \"\"\"\n",
    "        Evaluate a sampled block:\n",
    "          F(I_left, S_mid, J_right)\n",
    "        where each element of I_left and J_right is a multi-index prefix/suffix.\n",
    "\n",
    "        - I_left: list of length m, each a tuple of length (k-1)\n",
    "        - S_mid : list of physical indices tuples for the middle modes (usually 1 or 2 legs)\n",
    "        - J_right: list of length p, each a tuple of length (N-k) etc.\n",
    "\n",
    "        Returns array of shape (m, |S_mid|, p)\n",
    "        \"\"\"\n",
    "        m, s, p = len(I_left), len(S_mid), len(J_right)\n",
    "        out = np.empty((m, s, p), dtype=float) #create a 3D array of such ashape\n",
    "        for a, il in enumerate(I_left):\n",
    "            for b, sm in enumerate(S_mid):\n",
    "                for c, jr in enumerate(J_right):\n",
    "                    idx = il + sm + jr\n",
    "                    out[a, b, c] = self.entry(idx)   #generate all combinations of indices at (a,b,c) with indice sets (il, sm, jr)\n",
    "        return out\n",
    "\n",
    "\n",
    "def oracle_from_tt(cores: List[Array]) -> TensorOracle:\n",
    "    dims = tuple(G.shape[1] for G in cores) # collect second dimension of each core=dimension of each leg into a tuple\n",
    "    def _entry(idx: Tuple[int, ...]) -> float:\n",
    "        return tt_eval_entry(cores, idx) # takes a list/tuple of indices as input and returns the tensor entry value ny calling tt_eval\n",
    "    return TensorOracle(dims=dims, entry=_entry)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Quick sanity test\n",
    "# ----------------------------\n",
    "\n",
    "dims = (3, 3, 3, 3)\n",
    "ranks = (1, 2, 2, 2, 1)  # TT ranks\n",
    "gt_cores = tt_random_cores(dims, ranks, seed=42)\n",
    "F = oracle_from_tt(gt_cores)\n",
    "\n",
    "# test a random entry\n",
    "print(\"F(0,1,2,0) =\", F((0,1,2,0)))\n",
    "\n",
    "# materialize full tensor for small dims and compare\n",
    "full = tt_contract_all(gt_cores)\n",
    "print(\"Full tensor shape:\", full.shape)\n",
    "print(\"Check same entry:\", full[0,1,2,0])\n",
    "max_diff = 0.0\n",
    "for _ in range(20):\n",
    "    idx = tuple(np.random.randint(0, d) for d in dims)\n",
    "    diff = abs(F(idx) - full[idx])\n",
    "    max_diff = max(max_diff, diff)\n",
    "\n",
    "print(\"Max abs diff (20 samples):\", max_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7aa8ad-6a25-4d8a-85ad-6cd0d48c6871",
   "metadata": {},
   "source": [
    "Testing to see if the error is around 0 for TT with random matrix-core to mimic SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "197d2b74-440c-425d-a7a8-f030a71e0aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r = 6\n",
      "diag(D) = [-5.         -9.         -7.55555556  9.55882353 -7.404      -6.97855629]\n",
      "relative error = 1.1192760593717226\n",
      "diag(U1[:, :r]) = [1. 1. 1. 1. 1. 1.]\n",
      "Selected rank r = 4\n",
      "First few pivot rows: [ 0  3 14 22]\n",
      "First few pivot cols: [ 5 23 32 12]\n",
      "Relative LU factorization error: 1.1517735944386922\n",
      "Relative skeleton approx error: 1.077389632898274\n"
     ]
    }
   ],
   "source": [
    "def prrlu_complete_pivot_LDU_direct(\n",
    "    A: np.ndarray,\n",
    "    rmax: int,\n",
    "    eps: float = 1e-12,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]: #need 7 outputs, row col indices, row permutation matrix and col premutation matix, L,D and U \n",
    "    \"\"\"\n",
    "    Complete-pivoting rank-revealing LU, returned explicitly as L D U1.\n",
    "\n",
    "    Produces:\n",
    "        Pr @ A @ Pc ≈ L @ D @ U1\n",
    "\n",
    "    Shapes:\n",
    "        L  : (m, r)  unit-lower (rectangular)\n",
    "        D  : (r, r)  diagonal (pivots)\n",
    "        U1 : (r, n)  unit-upper in first r columns (rectangular)\n",
    "    \"\"\"\n",
    "    Awork = np.array(A, dtype=float, copy=True) # do a copy of the input matrix so we don't modify it\n",
    "    m, n = Awork.shape\n",
    "    rmax = min(rmax, m, n)\n",
    "\n",
    "    prow = np.arange(m) #tracks the current ordering of rows, expressed in original row labels, updated whenever rows are swapped\n",
    "    pcol = np.arange(n) # same for cols \n",
    "\n",
    "    piv_rows = []\n",
    "    piv_cols = []\n",
    "\n",
    "    pivot0 = None\n",
    "    r = 0 #prrLU steps, also the rank\n",
    "    # We'll store pivots (diagonal D) as a vector first, then make diag matrix.\n",
    "    d_list = []\n",
    "    \n",
    "    for k in range(rmax):\n",
    "        # Find largest pivot in submatrix starting at row/col k\n",
    "        sub = np.abs(Awork[k:, k:])\n",
    "        i_rel, j_rel = np.unravel_index(np.argmax(sub), sub.shape) #argmax to find largest element in sub matrix and use unravel to find its index\n",
    "        i = k + i_rel #true indices in the big matrix \n",
    "        j = k + j_rel\n",
    "        piv = Awork[i, j] #read the element value at pivot since we need it as stopping criterion and to vide the col/row by \n",
    "\n",
    "        if pivot0 is None:\n",
    "            pivot0 = abs(piv) if abs(piv) != 0 else 1.0 #Store scale as the magnitude of the first pivot. If the first pivot is zero (degenerate), use 1.0 to avoid multiplying by zero later.\n",
    "\n",
    "        if abs(piv) <= eps * pivot0:\n",
    "            break #stop if the remaining pivot is tiny compared to matrix scale \n",
    "\n",
    "        # Swap rows/cols to move pivot to (k,k)\n",
    "        if i != k:                                  #if i is not equal to k\n",
    "            Awork[[k, i], :] = Awork[[i, k], :]     #for kth and ith row and all columns\n",
    "            prow[[k, i]] = prow[[i, k]]             # swap i to k \n",
    "        if j != k:\n",
    "            Awork[:, [k, j]] = Awork[:, [j, k]]\n",
    "            pcol[[k, j]] = pcol[[j, k]]\n",
    "\n",
    "        piv_rows.append(prow[k])\n",
    "        piv_cols.append(pcol[k]) #pivot is now at (k,k) tge true original indices are prow[k] and pcol[k]\n",
    "\n",
    "        pivot = Awork[k, k]\n",
    "        if pivot == 0:\n",
    "            break\n",
    "\n",
    "        d_list.append(pivot) #add the pivot element to the list of ds and later turn it into matrix D \n",
    "\n",
    "        # 3) compute L multipliers below pivot: L[i,k] = A[i,k] / pivot\n",
    "        if k + 1 < m:\n",
    "            Awork[k+1:, k] /= pivot # for rows k+1 to the end and col k we divide by pivot to later store in L matrix \n",
    "\n",
    "        # 4) normalize pivot row to become U1 row:\n",
    "        #    U1[k, j] = A[k, j] / pivot, for j >= k\n",
    "        Awork[k, k:] /= pivot #same for U matrix but all cols\n",
    "        # Now Awork[k,k] should be 1 (unit diagonal for U1)\n",
    "        Awork[k, k] = 1.0\n",
    "\n",
    "        # 5) Schur complement update:\n",
    "        #    A22 -= L21 * (Dk * U1_12)\n",
    "        # Here:\n",
    "        #   L21 is Awork[k+1:, k]\n",
    "        #   U1_12 is Awork[k, k+1:]\n",
    "        #   Dk is pivot\n",
    "        if (k + 1 < m) and (k + 1 < n): #not at the last row or col\n",
    "            Awork[k+1:, k+1:] -= np.outer(Awork[k+1:, k], pivot * Awork[k, k+1:])#left is matrix A_22 the schur complement right is L21-U12, basically the formula for schur update A/[A_11]=(lower trig of L)D(upper trig of U)\n",
    "\n",
    "        r += 1\n",
    "\n",
    "    # Build outputs\n",
    "    piv_rows = np.array(piv_rows, dtype=int)\n",
    "    piv_cols = np.array(piv_cols, dtype=int)\n",
    "\n",
    "    # Permutation matrices (debugging)\n",
    "    Pr = np.eye(m)[np.argsort(prow)]\n",
    "    Pc = np.eye(n)[:, np.argsort(pcol)] #check to see if LDU is equal to Pr A Pc\n",
    "\n",
    "    if r == 0:\n",
    "        L = np.zeros((m, 0))\n",
    "        D = np.zeros((0, 0))\n",
    "        U1 = np.zeros((0, n))\n",
    "        return piv_rows, piv_cols, Pr, Pc, L, D, U1 #prrLU failed\n",
    "\n",
    "    # Extract L (unit-lower) from Awork: below diag in first r columns + identity\n",
    "    L = np.tril(Awork[:, :r], k=-1) + np.eye(m, r)\n",
    "\n",
    "    # Extract U1 (unit-upper) from Awork: first r rows, upper-triangular-ish\n",
    "    # Since we normalized pivot rows, Awork already stores U1 in those rows.\n",
    "    U1 = np.triu(Awork[:r, :], k=0)\n",
    "\n",
    "    # Build D\n",
    "    D = np.diag(np.array(d_list, dtype=float))\n",
    "\n",
    "    return piv_rows, piv_cols, Pr, Pc, L, D, U1\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Quick test\n",
    "# ----------------------------\n",
    "rng = np.random.default_rng(0)\n",
    "A = rng.integers(-5, 6, size=(10, 12)).astype(float)\n",
    "\n",
    "piv_r, piv_c, Pr, Pc, L, D, U1 = prrlu_complete_pivot_LDU_direct(A, rmax=6, eps=1e-12)\n",
    "\n",
    "A_perm = Pr @ A @ Pc\n",
    "rel_err = np.linalg.norm(A_perm - L @ D @ U1) / np.linalg.norm(A_perm)\n",
    "print(\"r =\", len(piv_r))\n",
    "print(\"diag(D) =\", np.diag(D) if D.size else D)\n",
    "print(\"relative error =\", rel_err)\n",
    "\n",
    "if D.size:\n",
    "    print(\"diag(U1[:, :r]) =\", np.diag(U1[:, :len(piv_r)]))\n",
    "\n",
    "def skeleton_approx_from_pivots(A: np.ndarray, piv_rows: np.ndarray, piv_cols: np.ndarray, reg: float = 0.0):\n",
    "    \"\"\"\n",
    "    Given pivot rows/cols, form a CI approximation:\n",
    "        A ≈ A[:,J] @ inv(A[I,J]) @ A[I,:]\n",
    "\n",
    "    Returns:\n",
    "        A_hat\n",
    "    \"\"\"\n",
    "    I, J = piv_rows, piv_cols\n",
    "    C = A[:, J]                 # m x r\n",
    "    R = A[I, :]                 # r x n\n",
    "    P = A[np.ix_(I, J)]         # r x r\n",
    "\n",
    "    if reg > 0:\n",
    "        P = P + reg * np.eye(P.shape[0])\n",
    "    X = np.linalg.solve(P, R)   # r x n\n",
    "    return C @ X\n",
    "\n",
    "# ----------------------------\n",
    "# Quick test of CI on a random matrix\n",
    "# ----------------------------\n",
    "rng = np.random.default_rng(0)\n",
    "A = rng.integers(-5, 6, size=(30, 40)).astype(float)\n",
    "\n",
    "\n",
    "piv_r, piv_c, Pr, Pc, L, D, U1 = prrlu_complete_pivot_LDU(A, rmax=4, eps=1e-12)\n",
    "\n",
    "\n",
    "print(\"Selected rank r =\", len(piv_r))\n",
    "print(\"First few pivot rows:\", piv_r[:5])\n",
    "print(\"First few pivot cols:\", piv_c[:5])\n",
    "\n",
    "# Check reconstruction quality of permuted LU: Pr*A*Pc ≈ L*U\n",
    "A_perm = Pr @ A @ Pc\n",
    "if len(piv_r) > 0:\n",
    "    rel_err = np.linalg.norm(A_perm - L @ D @ U1) / np.linalg.norm(A_perm)\n",
    "    print(\"Relative LU factorization error:\", rel_err)\n",
    "\n",
    "# Check skeleton approximation error using the same pivots       cigkl stiffness tensor\n",
    "if len(piv_r) > 0:\n",
    "    A_hat = skeleton_approx_from_pivots(A, piv_r, piv_c)\n",
    "    rel_err_skel = np.linalg.norm(A - A_hat) / np.linalg.norm(A) #entry difference then do root mean square and normalised\n",
    "    print(\"Relative skeleton approx error:\", rel_err_skel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca55c03-4d95-42d3-9d8e-2ca6c6a6701a",
   "metadata": {},
   "source": [
    "Logic: prrLU still requires the detail of all entries in a matrix therefore we choose sub matrix from the full matrix first and then apply prrLU\n",
    "Testing: use random matrix to test the accuracy of algorithm just in case if the implimentation is not bugged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ff8890b7-331a-4a1e-8993-19bfd2f930b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 2-site block matrix A with shape: (9, 9)\n",
      "Oracle calls used to build A: 0\n",
      "\n",
      "=== Decoding BEFORE pivot update ===\n",
      "\n",
      "Row ID → tensor multi-index (prefix, σ_k):\n",
      "row  0 -> (0, 0)\n",
      "row  1 -> (0, 1)\n",
      "row  2 -> (0, 2)\n",
      "row  3 -> (2, 0)\n",
      "row  4 -> (2, 1)\n",
      "row  5 -> (2, 2)\n",
      "row  6 -> (1, 0)\n",
      "row  7 -> (1, 1)\n",
      "row  8 -> (1, 2)\n",
      "\n",
      "Col ID → tensor multi-index (σ_{k+1}, suffix):\n",
      "col  0 -> (0, 0)\n",
      "col  1 -> (0, 2)\n",
      "col  2 -> (0, 1)\n",
      "col  3 -> (1, 0)\n",
      "col  4 -> (1, 2)\n",
      "col  5 -> (1, 1)\n",
      "col  6 -> (2, 0)\n",
      "col  7 -> (2, 2)\n",
      "col  8 -> (2, 1)\n",
      "Local discovered rank r = 2\n",
      "Pivot row ids (in A): [5 4]\n",
      "Pivot col ids (in A): [6 0]\n",
      "Updated I_k (prefix indices up to site k): [(2, 2), (2, 1)]\n",
      "Updated J_{k+1} (suffix indices from site k+1): [(2, 0), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MultiIndex = Tuple[int, ...]\n",
    "\n",
    "\n",
    "def decode_row_id(row_id: int, I_left: List[MultiIndex], n_k: int) -> MultiIndex:\n",
    "    \"\"\"\n",
    "    To get the index of I after prrLU\n",
    "    Row index in the reshaped 2-site matrix corresponds to (i_{k-1}, sigma_k).\n",
    "\n",
    "    We pack rows as:\n",
    "        row_id = a * n_k + sigma_k \n",
    "    where:\n",
    "        a is index into I_left\n",
    "        sigma_k in {0,...,n_k-1} a\n",
    "\n",
    "    Returns the new left multi-index i_k = (i_{k-1}, sigma_k).\n",
    "    \"\"\"\n",
    "    a = row_id // n_k\n",
    "    \"\"\"\n",
    "    // used for integer division\n",
    "    %for the remainder\n",
    "    \"\"\"\n",
    "    sigma_k = row_id % n_k\n",
    "    return I_left[a] + (sigma_k,)\n",
    "\n",
    "\n",
    "def decode_col_id(col_id: int, J_right: List[MultiIndex], n_kp1: int) -> MultiIndex:\n",
    "    \"\"\"\n",
    "    Column index corresponds to (sigma_{k+1}, j_{k+2}).\n",
    "\n",
    "    We pack cols as:\n",
    "        col_id = sigma_{k+1} * |J_right| + b\n",
    "        (swap the ordering copared to row id, since for row id the sigma is on the right side and for col id sigma is on the left\n",
    "    where:\n",
    "        b is index into J_right\n",
    "        sigma_{k+1} in {0,...,n_{k+1}-1}\n",
    "\n",
    "    Returns new right multi-index j_{k+1} = (sigma_{k+1}, j_{k+2}).\n",
    "    \"\"\"\n",
    "    R = len(J_right)\n",
    "    sigma_kp1 = col_id // R\n",
    "    b = col_id % R\n",
    "    return (sigma_kp1,) + J_right[b]\n",
    "\n",
    "\n",
    "def two_site_block_matrix(F: TensorOracle, I_left: List[MultiIndex], k: int, J_right: List[MultiIndex]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build the 2-site sampled block:\n",
    "        Π_k = F(I_{k-1}, sigma_k, sigma_{k+1}, J_{k+2})\n",
    "\n",
    "    where:\n",
    "      - I_left contains multi-indices of length k-1\n",
    "      - J_right contains multi-indices of length N-(k+1)\n",
    "      - k is 1-based site index in math; here we'll use 0-based Python index for cores.\n",
    "\n",
    "    Returns Π as a 2D matrix with shape:\n",
    "      rows = |I_left| * n_k\n",
    "      cols = n_{k+1} * |J_right|\n",
    "    \"\"\"\n",
    "    N = len(F.dims)\n",
    "    assert 0 <= k <= N-2\n",
    "    \"\"\"\n",
    "    sites are 0..N-1\n",
    "      this function builds the block for sites k and k+1 (so requires k <= N-2)\n",
    "      \"\"\"\n",
    "    n_k = F.dims[k]\n",
    "    n_kp1 = F.dims[k+1]\n",
    "\n",
    "    # Build lists of middle index tuples for the pair of the two free legs, same as two for loops\n",
    "    S_mid = [(s1, s2) for s1 in range(n_k) for s2 in range(n_kp1)]  # length n_k*n_{k+1}\n",
    "\n",
    "    # Query the oracle for the 3D block of shape (|I_left|, n_k*n_{k+1}, |J_right|)\n",
    "    block3 = F.block(I_left=I_left, S_mid=S_mid, J_right=J_right)\n",
    "\n",
    "    # Now reshape into a matrix with:\n",
    "    #   rows = |I_left| * n_k\n",
    "    #   cols = n_{k+1} * |J_right|\n",
    "    #\n",
    "    # block3 is indexed by:\n",
    "    #   block3[a, b, c]\n",
    "    # where b corresponds to (sigma_k, sigma_{k+1}) in S_mid order.\n",
    "    #\n",
    "    # We'll reshape by first reshaping the middle axis into (n_k, n_{k+1}):\n",
    "    m = len(I_left)\n",
    "    p = len(J_right)\n",
    "    block4 = block3.reshape(m, n_k, n_kp1, p)          # (m, n_k, n_{k+1}, p)\n",
    "    A = block4.reshape(m * n_k, n_kp1 * p)             # (m*n_k, n_{k+1}*p)\n",
    "    return A # The 2D matrix for prrLU\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Demo: one local 2-site update\n",
    "# ----------------------------\n",
    "\n",
    "def random_multi_indices(dims, r):\n",
    "    all_idx = list(itertools.product(*[range(d) for d in dims]))\n",
    "    random.shuffle(all_idx)\n",
    "    return all_idx[:r]\n",
    "\n",
    "N = len(F.dims)\n",
    "k = 1  # build block for sites 1 and 2 (0-based)\n",
    "\n",
    "# Initialize very small pivot sets (path-based):\n",
    "\n",
    "# For this local block, I_left should be length k (i.e. k sites on the left of site k since we start at k = 0)\n",
    "# But in math it is I_{k-1}. For k=1 (sites 1&2), k-1=0 so I_left is empty prefix.\n",
    "# for any k, we store multi-indices of length k (sites 0..k-1)\n",
    "# and J_right of length N-(k+2) (sites k+2..N-1)\n",
    "#\n",
    "# We'll start with 3 random indices\n",
    "I_left  = random_multi_indices(F.dims[:k], r=3) #first k tensor dimensions with three realisations (of 0,1,2) doesnt include kth \n",
    "J_right = random_multi_indices(F.dims[k+2:], r=3) #k+2th index to the end (include k+2th), also three realisations\n",
    "\n",
    "\n",
    "F.reset_counter()\n",
    "A = two_site_block_matrix(F, I_left, k, J_right) #construct sample matrix with I and J while loop over indices in two free legs\n",
    "\n",
    "print(\"Built 2-site block matrix A with shape:\", A.shape)\n",
    "print(\"Oracle calls used to build A:\", F.n_calls)\n",
    "print(\"\\n=== Decoding BEFORE pivot update ===\")\n",
    "\n",
    "n_k = F.dims[k]\n",
    "n_kp1 = F.dims[k+1]\n",
    "\n",
    "print(\"\\nRow ID → tensor multi-index (prefix, σ_k):\")\n",
    "for rid in range(A.shape[0]):\n",
    "    decoded = decode_row_id(rid, I_left, n_k)\n",
    "    print(f\"row {rid:2d} -> {decoded}\")\n",
    "\n",
    "print(\"\\nCol ID → tensor multi-index (σ_{k+1}, suffix):\")\n",
    "for cid in range(A.shape[1]):\n",
    "    decoded = decode_col_id(cid, J_right, n_kp1)\n",
    "    print(f\"col {cid:2d} -> {decoded}\")\n",
    "\n",
    "# Run prrLU on this small matrix\n",
    "piv_r, piv_c, Pr, Pc, L, D, U1 = prrlu_complete_pivot_LDU(A, rmax=4, eps=1e-12) \n",
    "\n",
    "print(\"Local discovered rank r =\", len(piv_r))\n",
    "print(\"Pivot row ids (in A):\", piv_r)\n",
    "print(\"Pivot col ids (in A):\", piv_c)\n",
    "\n",
    "# Decode pivot ids into NEW multi-index sets\n",
    "n_k = F.dims[k]\n",
    "n_kp1 = F.dims[k+1]\n",
    "\n",
    "I_k = [decode_row_id(rid, I_left, n_k) for rid in piv_r]          # multi-indices length k+1\n",
    "J_kp1 = [decode_col_id(cid, J_right, n_kp1) for cid in piv_c]     # multi-indices length N-(k+1)\n",
    "\n",
    "print(\"Updated I_k (prefix indices up to site k):\", I_k)\n",
    "print(\"Updated J_{k+1} (suffix indices from site k+1):\", J_kp1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a1d849-ed07-4a68-83cd-f92e26f03dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
